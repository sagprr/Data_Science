{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uu-4inG9-g_B",
        "outputId": "f94ad537-19bf-4572-8688-e7e1dbf31d0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\n",
            "\u001b[1m3423204/3423204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Gathering data using TensorFlow's utility function\n",
        "text_file = tf.keras.utils.get_file(\n",
        "    fname='fra-eng.zip',\n",
        "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\",\n",
        "    extract=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import pathlib\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "# Defining the path to the text file\n",
        "text_file = pathlib.Path(text_file).parent / 'fra.txt'\n",
        "\n",
        "def normalize(line):\n",
        "    # Normalize unicode characters, strip leading/trailing whitespace, convert to lowercase\n",
        "    line = unicodedata.normalize(\"NFKC\", line.strip().lower())\n",
        "    # Handle special characters and add start and end tokens for the target language (French)\n",
        "    line = re.sub(r\"^([^ \\w])(?!\\s)\", r\"\\1\", line)\n",
        "    line = re.sub(r\"(\\s[^ \\w])(?!\\s)\", r\"\\1\", line)\n",
        "    line = re.sub(r\"(?!\\s)([^ \\w])$\", r\"\\1\", line)\n",
        "    line = re.sub(r\"(?!\\s)([^ \\w]\\s)\", r\"\\1\", line)\n",
        "    eng, fre = line.split(\"\\t\")\n",
        "    fre = '[start] ' + fre + ' [end]'\n",
        "    return eng, fre\n",
        "\n",
        "# Read and normalize the text pairs\n",
        "with open(text_file) as fp:\n",
        "    text_pairs = [normalize(line) for line in fp]\n"
      ],
      "metadata": {
        "id": "8gMlJ9NN-l2p"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization and Statistics\n",
        "\n",
        "# Initialize sets to store unique tokens for English and French\n",
        "eng_tokens, fre_tokens = set(), set()\n",
        "# Initialize variables to store maximum sequence lengths\n",
        "eng_maxlen, fre_maxlen = 0, 0\n",
        "\n",
        "# Iterate through text pairs to tokenize and compute statistics\n",
        "for eng, fre in text_pairs:\n",
        "    eng_token, fre_token = eng.split(), fre.split()\n",
        "    eng_maxlen = max(eng_maxlen, len(eng_token))\n",
        "    fre_maxlen = max(fre_maxlen, len(fre_token))\n",
        "    eng_tokens.update(eng_token)\n",
        "    fre_tokens.update(fre_token)\n",
        "\n",
        "# Print statistics\n",
        "print(f\"Total tokens in English: {len(eng_tokens)}\")\n",
        "print(f\"Total tokens in French: {len(fre_tokens)}\")\n",
        "print(f\"Maximum length of English sequence: {eng_maxlen}\")\n",
        "print(f\"Maximum length of French sequence: {fre_maxlen}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dfSApvg_RZh",
        "outputId": "7ed15192-c4bf-4765-f9a4-ec974fbff71a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens in English: 25365\n",
            "Total tokens in French: 42027\n",
            "Maximum length of English sequence: 47\n",
            "Maximum length of French sequence: 56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Serialize preprocessed data for future use\n",
        "with open(\"text_pairs.pickle\", 'wb') as fp:\n",
        "    pickle.dump(text_pairs, fp)\n"
      ],
      "metadata": {
        "id": "mEasuRpF_Xb7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rWyQjURV_YIS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}