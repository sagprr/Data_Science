{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def encoder(input_shape, key_dim, ff_dim, dropout=0.1, prefix='enc', **kwargs):\n",
        "    # Define a Sequential model for the encoder\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Input(shape=input_shape, dtype='float32', name=f'{prefix}_in0'), # Input layer\n",
        "        self_attention(input_shape, prefix=prefix, key_dim=key_dim, mask=False, **kwargs), # Self-attention layer\n",
        "        feed_forward(input_shape, key_dim, ff_dim, dropout, prefix) # Feed-forward layer\n",
        "    ])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "nJLRLLIDfhvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1otc7aefaNC"
      },
      "outputs": [],
      "source": [
        "def decoder(input_shape, key_dim, ff_dim, dropout=0.1, prefix='dec', **kwargs):\n",
        "    # Define inputs for decoder\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32', name=f'{prefix}_in0')\n",
        "    context = tf.keras.layers.Input(shape=input_shape, dtype='float32', name=f'{prefix}_ctx0')\n",
        "\n",
        "    # Self-attention and cross-attention layers\n",
        "    att_model = self_attention(input_shape, key_dim=key_dim, mask=True, prefix=prefix, **kwargs)\n",
        "    cross_model = cross_attention(input_shape, input_shape, key_dim=key_dim, prefix=prefix, **kwargs)\n",
        "\n",
        "    # Feed-forward layer\n",
        "    ff_model = feed_forward(input_shape, key_dim, ff_dim, dropout, prefix)\n",
        "\n",
        "    # Connect layers\n",
        "    x = att_model(inputs)\n",
        "    x = cross_model([context, x])\n",
        "    output = ff_model(x)\n",
        "\n",
        "    # Define decoder model\n",
        "    model = tf.keras.Model(inputs=[inputs, context], outputs=output, name=prefix)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer(num_layers, num_heads, seq_length, key_dim, ff_dim, vocab_size_en, vocab_size_fr, dropout=0.1, name='transformer'):\n",
        "    # Define encoder and decoder inputs\n",
        "    input_enc = tf.keras.layers.Input(shape=(seq_length), dtype='int32', name='encode_inp')\n",
        "    input_dec = tf.keras.layers.Input(shape=(seq_length), dtype='int32', name='decode_inp')\n",
        "\n",
        "    # Positional embeddings for encoder and decoder inputs\n",
        "    emb_enc = PositionalEmbedding(seq_length, vocab_size_en, key_dim, name='embed_enc')\n",
        "    emb_dec = PositionalEmbedding(seq_length, vocab_size_fr, key_dim, name='embed_dec')\n",
        "\n",
        "    # Create encoder and decoder layers\n",
        "    encoders = [encoder(input_shape=(seq_length, key_dim), key_dim=key_dim, ff_dim=ff_dim, dropout=dropout, prefix=f\"enc{i}\", num_heads=num_heads)\n",
        "                for i in range(num_layers)]\n",
        "    decoders = [decoder(input_shape=(seq_length, key_dim), key_dim=key_dim, ff_dim=ff_dim, dropout=dropout, prefix=f\"dec{i}\", num_heads=num_heads)\n",
        "                for i in range(num_layers)]\n",
        "\n",
        "    # Final dense layer\n",
        "    final = tf.keras.layers.Dense(vocab_size_fr, name='linear')\n",
        "\n",
        "    # Apply encoder and decoder layers to inputs\n",
        "    x1 = emb_enc(input_enc)\n",
        "    x2 = emb_dec(input_dec)\n",
        "    for layer in encoders:\n",
        "        x1 = layer(x1)\n",
        "    for layer in decoders:\n",
        "        x2 = layer([x2, x1])\n",
        "\n",
        "    # Generate output\n",
        "    output = final(x2)\n",
        "\n",
        "    try:\n",
        "        del output.keras_mask\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Define transformer model\n",
        "    model = tf.keras.Model(inputs=[input_enc, input_dec], outputs=output, name=name)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "vDYF0GsxfbG_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "t9wd7k_6s6d8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}